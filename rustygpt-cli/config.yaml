server_port: 8080
database_url: postgres://tinroof:rusty@localhost/rusty_gpt
log_level: info
frontend_path: ../frontend/dist
llm:
  default_provider: llama_cpp
  models_directory: ./models
  default_chat_model: default
  default_embedding_model: null
  providers:
    llama_cpp:
      provider_type: llama_cpp
      enabled: true
      n_gpu_layers: 0
      n_threads: null
      additional_settings: {}
  models:
    default:
      path: models/default.gguf
      provider: llama_cpp
      display_name: Default Model
      description: Default language model for general tasks
      default_params:
        max_tokens: 512
        temperature: 0.7
        top_p: 0.9
        top_k: 40
        repetition_penalty: 1.1
        context_size: 2048
        batch_size: 512
      capabilities:
        text_generation: true
        text_embedding: false
        chat_format: true
        function_calling: false
        streaming: true
        supported_languages:
        - en
  global_settings:
    default_timeout: 30
    max_concurrent_requests: 4
    enable_model_caching: true
    cache_size_limit_mb: 4096
    enable_request_logging: true
    enable_metrics: true
