# RustyGPT Configuration Example
# This file demonstrates the complete configuration structure for RustyGPT
# Copy this to config.yaml and modify as needed

# Server Configuration
server_port: 8080
database_url: "postgres://tinroof:rusty@localhost/rusty_gpt"
log_level: "info"
frontend_path: "./rustygpt-web/dist"

# LLM Configuration
llm:
  # Default provider to use for LLM operations
  default_provider: "llama_cpp"

  # Directory where model files are stored
  models_directory: "./models"

  # Default models for different tasks
  default_chat_model: "llama2-7b-chat"
  default_embedding_model: "all-minilm-l6-v2"

  # Provider configurations
  providers:
    llama_cpp:
      provider_type: "llama_cpp"
      enabled: true
      n_gpu_layers: 0 # Set to higher values if you have GPU support
      n_threads: 4 # Number of CPU threads to use
      additional_settings: {}

    # Example of another provider (when implemented)
    candle:
      provider_type: "candle"
      enabled: false
      n_gpu_layers: 10
      n_threads: 8
      additional_settings:
        use_flash_attention: true

  # Model configurations
  models:
    llama2-7b-chat:
      path: "llama-2-7b-chat.Q4_K_M.gguf" # Relative to models_directory
      provider: "llama_cpp"
      display_name: "Llama 2 7B Chat"
      description: "A 7 billion parameter chat model fine-tuned for conversations"
      default_params:
        max_tokens: 512
        temperature: 0.7
        top_p: 0.9
        top_k: 40
        repetition_penalty: 1.1
        context_size: 4096
        batch_size: 512
      capabilities:
        text_generation: true
        text_embedding: false
        chat_format: true
        function_calling: false
        streaming: true
        supported_languages: ["en", "es", "fr", "de"]

    llama2-13b-chat:
      path: "/absolute/path/to/llama-2-13b-chat.Q4_K_M.gguf" # Absolute path example
      provider: "llama_cpp"
      display_name: "Llama 2 13B Chat"
      description: "A larger 13 billion parameter model for better performance"
      default_params:
        max_tokens: 1024
        temperature: 0.8
        top_p: 0.95
        top_k: 50
        repetition_penalty: 1.05
        context_size: 4096
        batch_size: 256
      capabilities:
        text_generation: true
        text_embedding: false
        chat_format: true
        function_calling: true # Supports function calling
        streaming: true
        supported_languages: ["en", "es", "fr", "de", "pt", "it"]

    all-minilm-l6-v2:
      path: "all-MiniLM-L6-v2.gguf"
      provider: "llama_cpp"
      display_name: "All-MiniLM L6 v2"
      description: "Sentence transformer model for embeddings"
      default_params:
        max_tokens: 128
        temperature: 0.0 # No randomness for embeddings
        top_p: 1.0
        top_k: 1
        repetition_penalty: 1.0
        context_size: 512
        batch_size: 1024
      capabilities:
        text_generation: false
        text_embedding: true
        chat_format: false
        function_calling: false
        streaming: false
        supported_languages: ["en"]

  # Global LLM settings
  global_settings:
    default_timeout: 30 # Timeout in seconds
    max_concurrent_requests: 4 # Maximum concurrent LLM requests
    enable_model_caching: true # Cache loaded models in memory
    cache_size_limit_mb: 4096 # Cache size limit in MB
    enable_request_logging: true # Log LLM requests for debugging
    enable_metrics: true # Collect performance metrics
